version: '3.9'

networks:
  lab-networks:
    driver: bridge


services:

  config-server:
    build:
      context: ./cloud/config-server
      dockerfile: Dockerfile
    container_name: config-server
    networks:
      - lab-networks
    ports:
      - "8888:8888"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 40s


  eureka-server:
    build:
      context: ./cloud/eureka-server
      dockerfile: Dockerfile
    container_name: eureka-server
    networks:
      - lab-networks
    ports:
      - "8761:8761"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8761/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 40s


  gateway:
    build:
      context: ./cloud/gateway
      dockerfile: Dockerfile
    container_name: gateway
    networks:
      - lab-networks
    ports:
      - "8082:8082"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
    depends_on:
      config-server:
        condition: service_healthy
      eureka-server:
        condition: service_healthy
      keycloak:
        condition: service_started
#      grafana:
#        condition: service_started


  dental-lab-service:
    build:
      context: ./dental-service
      dockerfile: Dockerfile
    container_name: dental-lab-service
    networks:
      - lab-networks
    ports:
      - "8000:8000"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
    depends_on:
      config-server:
        condition: service_healthy
      eureka-server:
        condition: service_healthy
      keycloak:
        condition: service_started
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
      minio:
        condition: service_started
      kafka:
        condition: service_started
#      grafana:
#        condition: service_started


  ui-application:
    build:
      context: ./ui-mvc-app
      dockerfile: Dockerfile
    container_name: ui-application
    networks:
      - lab-networks
    ports:
      - "8081:8081"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
    depends_on:
      config-server:
        condition: service_healthy
      gateway:
        condition: service_started
      keycloak:
        condition: service_started
      redis:
        condition: service_started


  telegram-bot:
    build:
      context: ./telegram-bot
      dockerfile: Dockerfile
    container_name: telegram-bot
    networks:
      - lab-networks
    ports:
      - "8001:8001"
    environment:
      - SPRING_PROFILES_ACTIVE=prod
    depends_on:
      config-server:
        condition: service_healthy
      gateway:
        condition: service_started
      redis:
        condition: service_started
      kafka:
        condition: service_started


  postgres:
    container_name: dental_db
    image: postgres:15.3-alpine
    networks:
      - lab-networks
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - PGDATA=/var/lib/postgresql/data
      - POSTGRES_DB=dental_lab
    ports:
      - '5432:5432'
    volumes:
      - ./volumes/postgresql/data:/var/lib/postgresql/data
      - ./create-db.sql:/docker-entrypoint-initdb.d/create_database.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 5s
      timeout: 5s
      retries: 5


  keycloak:
    container_name: dental_keycloak
    image: quay.io/keycloak/keycloak:25.0
    command:
      - start
    networks:
      - lab-networks
    environment:
      KC_HOSTNAME: host.docker.internal
      KC_HOSTNAME_PORT: 8080
      KC_HOSTNAME_STRICT_BACKCHANNEL: false
      KC_HTTP_ENABLED: true
      KC_HOSTNAME_STRICT_HTTPS: false
      KC_HEALTH_ENABLED: true
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres/keycloakdb
      KC_DB_USERNAME: postgres
      KC_DB_PASSWORD: postgres
      KC_FRONTEND_URL: http://localhost:8080
    ports:
      - '8080:8080'
    depends_on:
      postgres:
        condition: service_healthy
    volumes:
      - ./volumes/keycloak/data:/opt/keycloak/data
      - ./volumes/keycloak/themes:/opt/keycloak/themes


  kafka:
    container_name: dental_kafka
    image: bitnami/kafka:latest
    networks:
      - lab-networks
    ports:
      - '9092:9092'
    volumes:
      - "kafka_data:/bitnami"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9091,CONTROLLER://:9093,EXTERNAL://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://:9091,EXTERNAL://localhost:9092
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT


  minio:
    image: minio/minio:latest
    container_name: minio
    networks:
      - lab-networks
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server --console-address ":9001" /data
    volumes:
      - ./volumes/minio_data:/data


  redis:
    container_name: dental_redis
    image: redis:latest
    networks:
      - lab-networks
    ports:
      - '6379:6379'
    command: redis-server --save 20 1 --loglevel warning
    volumes:
      - ./volumes/redis/data:/data


# observability
  alloy:
    image: grafana/alloy:latest
    container_name: alloy
    ports:
      - "12345:12345"
      - "4317:4317"
      - "4318:4318"
    volumes:
      - ./config/alloy/config.alloy:/etc/alloy/config.alloy
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --storage.path=/var/lib/alloy/data
    networks:
      - lab-networks


  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
      - '--web.enable-remote-write-receiver'
    networks:
      - lab-networks


  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    ports:
      - "3100:3100"
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./config/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - ./volumes/loki-data:/loki
    networks:
      - lab-networks


  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    ports:
      - "3200:3200"
    volumes:
      - ./config/tempo/tempo.yaml:/etc/tempo/tempo.yaml
      - ./config/tempo/overrides.yaml:/etc/tempo/overrides.yaml
      - ./volumes/tempo:/var/tempo
    command: -config.file=/etc/tempo/tempo.yaml
    networks:
      - lab-networks


  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro
    networks:
      - lab-networks
    depends_on:
      alloy:
        condition: service_started
      prometheus:
        condition: service_started
      loki:
        condition: service_started
      tempo:
        condition: service_started


volumes:
  kafka_data:
    driver: local
  prometheus-data:
  loki-data:
  tempo-data:
  grafana-data: